# -*- coding: utf-8 -*-
"""ML-HW1-GermanCreditCard-AlperBingol.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F8kk_J_RbLWCe0XoS7ZlPtBLja5c97cJ

# Load the dataset
"""

from google.colab import drive
drive.mount('/content/drive')

# You can find the data under https://drive.google.com/drive/folders/1e550az93U3_kfRBbVY5PZnMKYwGYmHqi?usp=sharing

import pandas as pd
import numpy as np
import time

from os.path import join

path = "/content/drive/My Drive"

train_data = pd.read_csv(join(path, "train_data.csv"))
train_label = pd.read_csv(join(path, "train_label.csv"))

test_data = pd.read_csv(join(path, "test_data.csv"))
test_label = pd.read_csv(join(path, "test_label.csv"))

# show random samples from the training data
train_data.sample(15)
# One line of code

"""# Train Decision Tree with default parameters"""

from sklearn.tree import DecisionTreeClassifier

# Train decision tree using the whole training data with **entropy** criteria
s=time.time()
# One line of code
clf = DecisionTreeClassifier('entropy')
# One line of code
clf = clf.fit(train_data, train_label)
e=time.time()
print(e-s)


# Estimate the prediction of test data
test_pred = clf.predict(test_data)



# Calculate accuracy of test data
from sklearn.metrics import accuracy_score
TestAcc = accuracy_score(test_label, test_pred)
print("Testing Accuracy = %.5f%%" % (TestAcc * 100))

"""# FineTune Decision Tree parameters

1- Spliting dataset into train and validation
"""

# Split training data to 70% training and 30% validation
from sklearn.model_selection import train_test_split
x_train, x_val, y_train, y_val = train_test_split(train_data,train_label, test_size=0.3)

"""2- FineTune minimum sample split"""

min_samples_splits = range(2, 100)

train_results = []
val_results = []
for min_samples_split in min_samples_splits:
  
  # Fit the tree using the 70% portion of the training data
  # One line of code
  dt = DecisionTreeClassifier(min_samples_split=min_samples_split, criterion='entropy')
  # One line of code
  dt.fit(x_train,y_train)
  

  # Evaluate on Training set
  train_pred = dt.predict(x_train)
  train_acc = accuracy_score(y_train,train_pred)
  train_results.append(train_acc)
   

  # Evaluate on Validation set
  val_pred = dt.predict(x_val)
  val_acc = accuracy_score(y_val, val_pred)
  val_results.append(val_acc)
  print("For min split = %d, validation accuracy = %.5f%%" % (min_samples_split, accuracy_score(y_val,val_pred)*100))
        

# Ploting
import matplotlib.pyplot as plt

plt.plot(min_samples_splits, train_results, 'b')
plt.plot(min_samples_splits, val_results,'r')
plt.show()

# Choose the best minimum split sample based on the plot
Best_minSampl = val_results.index(max(val_results))+2
#+2 is for indexing
print("Best minimum sample split: ",Best_minSampl)

# Train decision tree using the full training data and the best minimum split sample
clf = DecisionTreeClassifier(min_samples_split=Best_minSampl, criterion= 'entropy')
clf = clf.fit(train_data, train_label)

# Estimate the prediction of the test data
test_pred = clf.predict(test_data)

# Calculate accuracy of test data
TestAcc = accuracy_score(test_label, test_pred)
print("Testing Accuracy = %.5f%%" % (TestAcc * 100))

confusion_matrix(test_label,test_pred)

"""# Now, apply the same procedure but using KNN instead of decision tree 

# For finetuning, find the best value of K to use with this dataset.
"""

# Write your code here

from sklearn.neighbors import KNeighborsClassifier

# initialize the values of k to be a list of odd numbers between 1 and 30
kVals = range(1, 30, 2)

# Save the accuracies of each value of kVal in [accuracies] variable
# hint: you can use accuracies.append(...) function inside the loop
accuracies = []

# loop over values of k for the k-Nearest Neighbor classifier
for k in kVals:
  # Follow what we did in decision tree part
  model=KNeighborsClassifier(n_neighbors=k)
  model.fit(x_train,y_train.values.ravel())
  score=model.score(x_val,y_val)
  accuracies.append(score)

# Train KNN using the full training data with the best K that you found
i=np.argmax(accuracies)
print("best k = %d with %.5f%% validation accuracy" % (kVals[i], accuracies[i] * 100))

s=time.time()

model=KNeighborsClassifier(n_neighbors=kVals[i])
model.fit(train_data,train_label.values.ravel())
e=time.time()
prediction=model.predict(test_data)


# Testing
test_Accuracy=accuracy_score(test_label,prediction)
print("Testing accuracy = %.5f%%" % (test_Accuracy*100))

from sklearn.metrics import confusion_matrix
confusion_matrix(test_label,prediction)
print(e-s)

"""# Bonus

# Apply gridsearch using decision tree on any hyperparameter(s) of your choice, you have to beat your previous obtained accuracies to get the bonus
"""

# Write your code here
parameters={'min_samples_split' : range(2,100,2),'max_depth': range(1,20,2),'criterion': ["gini", "entropy"],'splitter':["best", "random"]}


from sklearn.model_selection import GridSearchCV
clf = DecisionTreeClassifier()
grid_clf_acc = GridSearchCV(clf, param_grid = parameters)
grid_clf_acc.fit(x_train, y_train)


#Predict values based on new parameters
y_pred_acc = grid_clf_acc.predict(x_val)

# New Model Evaluation metrics 
print('Accuracy Score : ' + str(accuracy_score(y_val,y_pred_acc)))
confusion_matrix(y_val,y_pred_acc)

"""# Report: Write a summary of your approach to this problem; this should be like an abstract of a paper or the executive summary (you aim for clarity and passing on information, not going to details about known facts such as what decision trees are, assuming they are known to people in your research area).

Must include statements such as:


*   Include the problem definition: 1-2 lines
*   Talk about train/val/test sets, size and how split.
*   State what your test results are with the chosen method, parameters: e.g. "We have obtained the best results with the ….. classifier (parameters=....) , giving classification accuracy of …% on test data…."
*   Comment on the speed of the algorithms and anything else that you deem important/interesting (e.g. confusion matrix)

# Write your report in this cell

The problem is to determine whether a bank should give loan to their customers according to customer's history. The dataset includes features about customers and labels which indicates decision for giving loans to customers. To reach our goal Decision Tree Classifier and KNN Classifier are used.The test and train data have been given to us and they are splitted as 207 and 793 rows respectively. However, we still need to split the data for validation and training. In the algorithms we splitted data randomly by using model selection algoritm which named train_test_split() in sklearn module with split size 0.3 that means 30% of all datas will be reserved by validation. In Decision Tree Classifier we obtained the best results with min_sample_splits and criterion='entropy' parameters. The validation accuracy is 73.10% with given parameters. Testing  accuracy is 66.18% with best minimum split 31 on test data. At the same time, for KNN Classifier we obtained best results with n_neighbors parameter. Our testing accuracy is 69.56% on test data with 21 k while validation accuracy is 69.32%. The speed of the algoritms have been obtained by time() function in time module. For the Decision tree algorithmn the elapsed time is calculated as "Elapsed time:  0.011122465133666992" with the "Testing Accuracy = 66.18%" and confision matrix "array([[ 25,  37],
[ 21, 124]])". Whereas, for the kNN classifier the results are "Elapsed time: 0.003773212432861328", "Testing accuracy = 69.56%" and confision matrix "array([[  4,  58],
[  5, 140]])".
"""